# TODO
# Use this cell to complete your solution
# from pyspark.sql import functions as F
# import pyspark.sql.functions as f
# from pyspark.sql.types import TimestampType

# df =  spark.read.option("inferSchema",True).json('/user/muhasiddiqui@deloitte.com/dbacademy/developer-foundations-capstone/raw/orders/stream/order_0612a18b-0cc7-43ea-9f5b-155aad967cb9_2020-01-01.json')
# schema =  df.schema

# df = spark.readStream.schema(schema).option("maxFilesPerTrigger", 1).json(stream_path)


# df = df.withColumn("ingested_at", F.current_timestamp())
# df = df.withColumn("ingest_file_name", F.input_file_name())
# df = df.withColumn("submitted_at", f.from_unixtime("submittedAt")).withColumn("submitted_at", f.col("submittedAt").cast("Timestamp"))
# df = df.withColumn("submitted_yyyy_mm", f.date_format("submittedAt", "yyyy-MM"))
# df = df.drop("submittedAt")


############# USE EXPLODE TO EXTRACT FROM ARRAY INTO COLUMNS ##################################
# df = df.withColumn('exploded', F.explode('products'))
# df=df.withColumn('productId', F.col('exploded').getItem("productId")).withColumn('quantity', F.col('exploded').getItem("quantity")).withColumn('soldPrice', F.col('exploded').getItem("soldPrice"))
################ OR USE GET ITEM FUNCTION #####################################################
# df = df.withColumn("product_id", df["products"].getItem(0).productId).withColumn("quantity", df["products"].getItem(0).quantity).withColumn("soldPrice", df["products"].getItem(0).soldPrice)
############### BUT WE DONT NEED IT IN THIS EXERCISE ##########################################

# df = df.drop("exploded")
# df = df.drop("quantity")
# df = df.drop("soldPrice")
# df = df.drop("productId")
# df = df.drop("products")
# df = df.withColumn("shipping_address_address",F.col("shippingAddress.address")).withColumn("shipping_address_attention",F.col("shippingAddress.attention")).withColumn("shipping_address_city",F.col("shippingAddress.city")).withColumn("shipping_address_state",F.col("shippingAddress.state")).withColumn("shipping_address_zip",F.col("shippingAddress.zip"))

# df = df.drop("shippingAddress")

# df = df.withColumnRenamed("customerId","customer_id").withColumnRenamed("orderId","order_id").withColumnRenamed("salesRepId","sales_rep_id")
# df = df.withColumn("shipping_address_zip", F.col("shipping_address_zip").cast("Integer"))

# display(df)
# df.printSchema()

# df.writeStream.format('delta').queryName(orders_table).partitionBy("submitted_yyyy_mm").outputMode("append").option("checkpointLocation", orders_checkpoint_path).table(orders_table)
